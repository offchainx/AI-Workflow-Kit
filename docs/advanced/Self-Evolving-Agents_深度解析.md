# ğŸ§  Self-Evolving Agents æ·±åº¦è§£æ

**é¡¹ç›®**: [EvoAgentX/Awesome-Self-Evolving-Agents](https://github.com/EvoAgentX/Awesome-Self-Evolving-Agents)
**ä¸»é¢˜**: è‡ªè¿›åŒ– AI Agent çš„å®Œæ•´ç ”ç©¶ç»¼è¿°
**é‡è¦æ€§**: â­â­â­â­â­ (AI Agent é¢†åŸŸçš„å‰æ²¿ç ”ç©¶é›†åˆ)

---

## ğŸ“‹ ç›®å½•

1. [æ ¸å¿ƒæ¦‚å¿µ](#1-æ ¸å¿ƒæ¦‚å¿µ)
2. [ä¸ºä»€ä¹ˆé‡è¦](#2-ä¸ºä»€ä¹ˆé‡è¦)
3. [ä¸‰å¤§ä¼˜åŒ–é¢†åŸŸ](#3-ä¸‰å¤§ä¼˜åŒ–é¢†åŸŸ)
4. [æŠ€æœ¯å®ç°æ–¹æ³•](#4-æŠ€æœ¯å®ç°æ–¹æ³•)
5. [å¼€æºæ¡†æ¶](#5-å¼€æºæ¡†æ¶)
6. [å®é™…åº”ç”¨åœºæ™¯](#6-å®é™…åº”ç”¨åœºæ™¯)
7. [ä¸ä½ çš„é¡¹ç›®å…³è”](#7-ä¸ä½ çš„é¡¹ç›®å…³è”)
8. [å­¦ä¹ è·¯å¾„](#8-å­¦ä¹ è·¯å¾„)

---

## 1. æ ¸å¿ƒæ¦‚å¿µ

### ä»€ä¹ˆæ˜¯ Self-Evolving Agentsï¼Ÿ

**å®šä¹‰**ï¼šèƒ½å¤Ÿé€šè¿‡ä¸ç¯å¢ƒäº¤äº’è‡ªä¸»æ”¹è¿›çš„ AI ç³»ç»Ÿ

```
ä¼ ç»Ÿ AI Agent                Self-Evolving Agent
    â†“                              â†“
å›ºå®šèƒ½åŠ›                        æŒç»­è¿›åŒ–
    â†“                              â†“
éœ€è¦é‡æ–°è®­ç»ƒ                   è‡ªä¸»å­¦ä¹ ä¼˜åŒ–
    â†“                              â†“
é™æ€è¡Œä¸º                        åŠ¨æ€é€‚åº”
```

### æ ¸å¿ƒç‰¹å¾

1. **è‡ªä¸»æ€§ (Autonomy)**
   - æ— éœ€äººå·¥å¹²é¢„
   - è‡ªåŠ¨å‘ç°é—®é¢˜
   - è‡ªåŠ¨ä¼˜åŒ–è§£å†³æ–¹æ¡ˆ

2. **æŒç»­æ€§ (Lifelong Learning)**
   - ä¸æ–­ç§¯ç´¯ç»éªŒ
   - è®°å¿†é‡è¦ä¿¡æ¯
   - é¿å…ç¾éš¾æ€§é—å¿˜

3. **é€‚åº”æ€§ (Adaptability)**
   - åº”å¯¹æ–°åœºæ™¯
   - è°ƒæ•´ç­–ç•¥
   - ä¼˜åŒ–å·¥ä½œæµ

4. **è¿›åŒ–æ€§ (Evolution)**
   - è¡Œä¸ºæ”¹è¿›
   - çŸ¥è¯†æ‰©å±•
   - èƒ½åŠ›æå‡

---

## 2. ä¸ºä»€ä¹ˆé‡è¦ï¼Ÿ

### å½“å‰ AI Agent çš„å±€é™

```
ä¼ ç»Ÿ AI Agent çš„é—®é¢˜ï¼š

é—®é¢˜ 1: å›ºå®šèƒ½åŠ›
â”œâ”€ ChatGPT åªèƒ½èŠå¤©
â”œâ”€ GitHub Copilot åªèƒ½å†™ä»£ç 
â””â”€ æ— æ³•è‡ªä¸»å­¦ä¹ æ–°æŠ€èƒ½

é—®é¢˜ 2: éœ€è¦é‡æ–°è®­ç»ƒ
â”œâ”€ æ¯æ¬¡æ”¹è¿›éœ€è¦æ”¶é›†æ•°æ®
â”œâ”€ è®­ç»ƒæˆæœ¬é«˜æ˜‚
â””â”€ éƒ¨ç½²å‘¨æœŸé•¿

é—®é¢˜ 3: æ— æ³•é€‚åº”æ–°åœºæ™¯
â”œâ”€ é‡åˆ°æœªçŸ¥é—®é¢˜å°±å¤±è´¥
â”œâ”€ æ— æ³•ä»é”™è¯¯ä¸­å­¦ä¹ 
â””â”€ ç¼ºä¹é•¿æœŸè®°å¿†
```

### Self-Evolving Agents çš„é©å‘½æ€§

```
è‡ªè¿›åŒ– Agent çš„ä¼˜åŠ¿ï¼š

âœ… å®æ—¶é€‚åº”
   ç”¨æˆ·: "å¸®æˆ‘åˆ†æè¿™ä¸ªæ–°æ ¼å¼çš„æ•°æ®"
   Agent: è‡ªåŠ¨å­¦ä¹ æ•°æ®æ ¼å¼ â†’ ç”Ÿæˆåˆ†ææ–¹æ³• â†’ å®Œæˆä»»åŠ¡

âœ… æŒç»­æ”¹è¿›
   ç¬¬ 1 æ¬¡: å‡†ç¡®ç‡ 70%
   ç¬¬ 10 æ¬¡: å‡†ç¡®ç‡ 85%
   ç¬¬ 100 æ¬¡: å‡†ç¡®ç‡ 95%ï¼ˆæ— éœ€é‡æ–°è®­ç»ƒï¼ï¼‰

âœ… è·¨åŸŸè¿ç§»
   Agent åœ¨ç¼–ç¨‹ä»»åŠ¡ä¸­å­¦åˆ°çš„æ¨¡å¼ â†’ åº”ç”¨åˆ°æ•°æ®åˆ†æ
   Agent åœ¨è‹±æ–‡ä»»åŠ¡ä¸­å­¦åˆ°çš„æ–¹æ³• â†’ è¿ç§»åˆ°ä¸­æ–‡
```

---

## 3. ä¸‰å¤§ä¼˜åŒ–é¢†åŸŸ

è¿™ä¸ªé¡¹ç›®å°†è‡ªè¿›åŒ–æŠ€æœ¯åˆ†ä¸ºä¸‰å¤§ç±»ï¼š

### 3.1 å• Agent ä¼˜åŒ– (Single-Agent Optimization)

**ç›®æ ‡**: è®©ä¸€ä¸ª Agent å˜å¾—æ›´å¼º

```
ä¼˜åŒ–æ–¹å‘ï¼š
â”œâ”€ 1. LLM è¡Œä¸ºä¼˜åŒ–
â”‚   â”œâ”€ è®­ç»ƒæ—¶ä¼˜åŒ–ï¼ˆTraining-basedï¼‰
â”‚   â”‚   â”œâ”€ ç›‘ç£å¾®è°ƒï¼šToRA, STaR, MuMath-Code
â”‚   â”‚   â”œâ”€ å¼ºåŒ–å­¦ä¹ ï¼šSelf-Rewarding LMs, R-Zero, SPIRAL
â”‚   â”‚   â””â”€ è¿‡ç¨‹å¥–åŠ±å»ºæ¨¡ï¼šCodeRL, CodeT5+
â”‚   â”‚
â”‚   â””â”€ æµ‹è¯•æ—¶ä¼˜åŒ–ï¼ˆTest-timeï¼‰
â”‚       â”œâ”€ åé¦ˆä¼˜åŒ–ï¼šCodeT, Math-Shepherd
â”‚       â”œâ”€ æœç´¢ç­–ç•¥ï¼šTree of Thoughts, Forest-of-Thought
â”‚       â””â”€ æ¨ç†å¢å¼ºï¼šChain-of-Thought, Self-Consistency
â”‚
â”œâ”€ 2. Prompt å·¥ç¨‹ä¼˜åŒ–
â”‚   â”œâ”€ ç¼–è¾‘å¼ï¼ˆEdit-Basedï¼‰ï¼šGPS, GrIPS, TEMPERA
â”‚   â”œâ”€ è¿›åŒ–å¼ï¼ˆEvolutionaryï¼‰ï¼šEvoPrompt, Promptbreeder
â”‚   â”œâ”€ ç”Ÿæˆå¼ï¼ˆGenerativeï¼‰ï¼šOPRO, PromptAgent
â”‚   â””â”€ æ¢¯åº¦å¼ï¼ˆGradient-Basedï¼‰ï¼šTextGrad, REVOLVE
â”‚
â”œâ”€ 3. è®°å¿†ç³»ç»Ÿä¼˜åŒ–
â”‚   â”œâ”€ é•¿æœŸè®°å¿†ï¼šMemento, A-MEM
â”‚   â”œâ”€ åŠ¨æ€æ•´åˆï¼šM3-Agent
â”‚   â””â”€ å¥–åŠ±é©±åŠ¨ï¼šMemory-R1
â”‚
â”œâ”€ 4. å·¥å…·ä½¿ç”¨ä¼˜åŒ–
â”‚   â”œâ”€ ç›‘ç£å­¦ä¹ ï¼šToolLLM, BUTTON
â”‚   â”œâ”€ å¼ºåŒ–å­¦ä¹ ï¼šReTool, ToolRL
â”‚   â””â”€ åŠŸèƒ½è¿›åŒ–ï¼šCREATOR, Alita
â”‚
â””â”€ 5. ç»Ÿä¸€ä¼˜åŒ–
    â””â”€ å¤šç»´åº¦è”åˆä¼˜åŒ–
```

### 3.2 å¤š Agent ç³»ç»Ÿ (Multi-Agent Systems)

**ç›®æ ‡**: è®©å¤šä¸ª Agent åä½œæ›´é«˜æ•ˆ

```
å¤š Agent ä¼˜åŒ–ï¼š

1. è‡ªåŠ¨æ„å»º Agent å›¢é˜Ÿ
   â”œâ”€ MetaAgentï¼ˆæœ‰é™çŠ¶æ€æœºç»„åˆï¼‰
   â”œâ”€ AFlowï¼ˆè‡ªåŠ¨ç”Ÿæˆå·¥ä½œæµï¼‰
   â””â”€ ScoreFlowï¼ˆåå¥½ä¼˜åŒ–åè°ƒï¼‰

2. å·¥ä½œæµç¼–æ’
   â”œâ”€ AutoGenï¼ˆå¾®è½¯ï¼‰
   â”œâ”€ MetaGPTï¼ˆå¤šè§’è‰²åä½œï¼‰
   â”œâ”€ AgentVerseï¼ˆAgent ç¤¾ç¾¤ï¼‰
   â””â”€ GPTSwarmï¼ˆç¾¤ä½“æ™ºèƒ½ï¼‰

3. æ¶Œç°è¡Œä¸º
   â”œâ”€ Agent äº¤äº’äº§ç”Ÿæ–°èƒ½åŠ›
   â”œâ”€ é›†ä½“å†³ç­–ä¼˜äºä¸ªä½“
   â””â”€ è‡ªç»„ç»‡å’Œè‡ªé€‚åº”

4. åˆ†å¸ƒå¼å†³ç­–
   â”œâ”€ ä»»åŠ¡åˆ†è§£
   â”œâ”€ å¹¶è¡Œæ‰§è¡Œ
   â””â”€ ç»“æœèšåˆ
```

**å®é™…ä¾‹å­**ï¼š

```
åœºæ™¯ï¼šå¼€å‘ä¸€ä¸ª Web åº”ç”¨

ä¼ ç»Ÿæ–¹æ³•ï¼š
äººç±»å¼€å‘è€… â†’ å†™ä»£ç  â†’ æµ‹è¯• â†’ éƒ¨ç½²

å¤š Agent æ–¹æ³•ï¼š
PM Agent: åˆ†æéœ€æ±‚ â†’ ç”Ÿæˆ PRD
    â†“
Architect Agent: è®¾è®¡æ¶æ„ â†’ é€‰æ‹©æŠ€æœ¯æ ˆ
    â†“
Dev Agent: ç¼–å†™ä»£ç  â†’ å®ç°åŠŸèƒ½
    â†“
QA Agent: æµ‹è¯•ä»£ç  â†’ å‘ç° bug
    â†“
Dev Agent: ä¿®å¤ bug â†’ ä¼˜åŒ–æ€§èƒ½
    â†“
Deploy Agent: éƒ¨ç½²åˆ°ç”Ÿäº§ç¯å¢ƒ
```

### 3.3 é¢†åŸŸç‰¹å®šä¼˜åŒ– (Domain-Specific)

**ç›®æ ‡**: é’ˆå¯¹ç‰¹å®šé¢†åŸŸçš„ä¼˜åŒ–

```
ä¸“ä¸šé¢†åŸŸï¼š

1. æ•°å­¦æ¨ç†
   â”œâ”€ å®šç†è¯æ˜ï¼šMA-LoT
   â”œâ”€ é—®é¢˜æ±‚è§£ï¼šToRA, MuMath-Code
   â””â”€ ç¬¦å·è®¡ç®—ä¼˜åŒ–

2. è½¯ä»¶å¼€å‘
   â”œâ”€ è‡ªåŠ¨ç¼–ç¨‹ï¼šR&D-Agent
   â”œâ”€ ä»£ç å®¡æŸ¥ï¼šMAS-GPT
   â””â”€ æµ‹è¯•ç”Ÿæˆ

3. ç§‘å­¦ç ”ç©¶
   â”œâ”€ å®éªŒè®¾è®¡
   â”œâ”€ æ•°æ®åˆ†æ
   â””â”€ å‡è®¾éªŒè¯

4. ä¸šåŠ¡æµç¨‹
   â”œâ”€ æ–‡æ¡£å¤„ç†
   â”œâ”€ æ•°æ®æå–
   â””â”€ å†³ç­–æ”¯æŒ
```

---

## 4. æŠ€æœ¯å®ç°æ–¹æ³•

### 4.1 LLM è¡Œä¸ºä¼˜åŒ–è¯¦è§£

#### è®­ç»ƒæ—¶ä¼˜åŒ– (Training-Based)

**æ–¹æ³• 1: ç›‘ç£å¾®è°ƒ (Supervised Fine-Tuning)**

```python
# ä¼ªä»£ç ç¤ºä¾‹
class SelfEvolvingAgent:
    def __init__(self, base_model):
        self.model = base_model
        self.experience_buffer = []

    def execute_task(self, task):
        # æ‰§è¡Œä»»åŠ¡
        result = self.model.generate(task)

        # æ”¶é›†åé¦ˆ
        feedback = self.get_feedback(result)

        # å­˜å‚¨ç»éªŒ
        self.experience_buffer.append({
            'task': task,
            'result': result,
            'feedback': feedback
        })

        # å®šæœŸè‡ªæˆ‘è®­ç»ƒ
        if len(self.experience_buffer) > 100:
            self.self_train()

        return result

    def self_train(self):
        # ä»æˆåŠŸçš„ç»éªŒä¸­å­¦ä¹ 
        successful_examples = [
            exp for exp in self.experience_buffer
            if exp['feedback'].score > 0.8
        ]

        # å¾®è°ƒæ¨¡å‹
        self.model.fine_tune(successful_examples)
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **ToRA** (Tool-integrated Reasoning Agent)
  - å­¦ä¹ ä½•æ—¶ä½¿ç”¨å·¥å…·
  - å­¦ä¹ å¦‚ä½•ç»„åˆå·¥å…·
  - å­¦ä¹ è§£é‡Šå·¥å…·è¾“å‡º

- **STaR** (Self-Taught Reasoner)
  - ä»è‡ªå·±çš„æˆåŠŸæ¨ç†ä¸­å­¦ä¹ 
  - ç”Ÿæˆè®­ç»ƒæ•°æ®
  - è¿­ä»£æ”¹è¿›

**æ–¹æ³• 2: å¼ºåŒ–å­¦ä¹  (Reinforcement Learning)**

```python
# RL-based Self-Evolution
class RLAgent:
    def __init__(self):
        self.policy = PolicyNetwork()
        self.value = ValueNetwork()

    def learn_from_interaction(self, environment):
        state = environment.reset()
        episode_data = []

        while not done:
            # é€‰æ‹©åŠ¨ä½œ
            action = self.policy.select_action(state)

            # æ‰§è¡ŒåŠ¨ä½œ
            next_state, reward, done = environment.step(action)

            # è®°å½•ç»éªŒ
            episode_data.append((state, action, reward))

            state = next_state

        # æ›´æ–°ç­–ç•¥
        self.update_policy(episode_data)

    def update_policy(self, episode_data):
        # è®¡ç®—å›æŠ¥
        returns = self.compute_returns(episode_data)

        # ç­–ç•¥æ¢¯åº¦æ›´æ–°
        self.policy.update(episode_data, returns)
        self.value.update(episode_data, returns)
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **Self-Rewarding LMs** (è‡ªæˆ‘å¥–åŠ±è¯­è¨€æ¨¡å‹)
  - Agent è‡ªå·±è¯„åˆ¤è¾“å‡ºè´¨é‡
  - æ— éœ€äººå·¥æ ‡æ³¨å¥–åŠ±
  - æŒç»­è‡ªæˆ‘æ”¹è¿›

- **R-Zero** (é›¶åˆå§‹æ•°æ®å¼ºåŒ–å­¦ä¹ )
  - å®Œå…¨ä»é›¶å¼€å§‹
  - é€šè¿‡è‡ªæˆ‘å¯¹å¼ˆå­¦ä¹ 
  - AlphaGo æ€æƒ³åº”ç”¨åˆ°è¯­è¨€æ¨¡å‹

#### æµ‹è¯•æ—¶ä¼˜åŒ– (Test-Time)

**æ–¹æ³• 1: åé¦ˆé©±åŠ¨ä¼˜åŒ–**

```python
class FeedbackDrivenAgent:
    def solve_with_feedback(self, problem):
        max_iterations = 5

        for i in range(max_iterations):
            # ç”Ÿæˆè§£å†³æ–¹æ¡ˆ
            solution = self.generate_solution(problem)

            # è·å–åé¦ˆ
            feedback = self.get_feedback(solution)

            if feedback.is_correct:
                return solution

            # æ ¹æ®åé¦ˆæ”¹è¿›
            problem = self.refine_problem(problem, feedback)

        return solution

    def get_feedback(self, solution):
        # å¯ä»¥æ˜¯ï¼š
        # 1. ä»£ç æ‰§è¡Œç»“æœ
        # 2. å•å…ƒæµ‹è¯•
        # 3. ç”¨æˆ·åé¦ˆ
        # 4. å¦ä¸€ä¸ª LLM çš„è¯„åˆ¤
        return Feedback(...)
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **CodeT** (Code Testing)
  - ç”Ÿæˆä»£ç 
  - è¿è¡Œæµ‹è¯•
  - æ ¹æ®é”™è¯¯ä¿®å¤
  - è¿­ä»£ç›´åˆ°é€šè¿‡

- **Math-Shepherd** (æ•°å­¦é—®é¢˜æ±‚è§£)
  - é€æ­¥éªŒè¯æ¨ç†
  - å‘ç°é”™è¯¯æ­¥éª¤
  - é‡æ–°æ¨ç†

**æ–¹æ³• 2: æœç´¢ç­–ç•¥ä¼˜åŒ–**

```python
# Tree of Thoughts
class TreeOfThoughtsAgent:
    def solve(self, problem):
        # åˆå§‹åŒ–æœç´¢æ ‘
        root = ThoughtNode(problem)

        # å¹¿åº¦ä¼˜å…ˆæœç´¢
        frontier = [root]

        while frontier:
            node = frontier.pop(0)

            # ç”Ÿæˆå¤šä¸ªæ€è€ƒåˆ†æ”¯
            thoughts = self.generate_thoughts(node.state, num=3)

            for thought in thoughts:
                # è¯„ä¼°æ¯ä¸ªæ€è€ƒçš„è´¨é‡
                score = self.evaluate_thought(thought)

                if score > threshold:
                    child = ThoughtNode(thought, parent=node)
                    frontier.append(child)

                    # å¦‚æœæ‰¾åˆ°è§£å†³æ–¹æ¡ˆ
                    if self.is_solution(child):
                        return child.get_path()

        return None
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **Tree of Thoughts** (æ€ç»´æ ‘)
  - æ¢ç´¢å¤šæ¡æ¨ç†è·¯å¾„
  - è¯„ä¼°æ¯æ¡è·¯å¾„çš„è´¨é‡
  - é€‰æ‹©æœ€ä½³è·¯å¾„

- **Forest-of-Thought** (æ€ç»´æ£®æ—)
  - å¹¶è¡Œæ¢ç´¢å¤šæ£µæ ‘
  - é›†æˆå¤šä¸ªè§£å†³æ–¹æ¡ˆ
  - æ›´å¼ºçš„é²æ£’æ€§

### 4.2 Prompt ä¼˜åŒ–è¯¦è§£

#### å››å¤§ä¼˜åŒ–èŒƒå¼

**1. ç¼–è¾‘å¼ (Edit-Based)**

```python
# GPS (Gradual Prompt Search)
class EditBasedPromptOptimizer:
    def optimize(self, initial_prompt, task):
        prompt = initial_prompt

        for iteration in range(10):
            # è¯„ä¼°å½“å‰ prompt
            performance = self.evaluate(prompt, task)

            # ç”Ÿæˆç¼–è¾‘å»ºè®®
            edits = self.suggest_edits(prompt, performance)

            # åº”ç”¨æœ€ä½³ç¼–è¾‘
            new_prompt = self.apply_best_edit(prompt, edits)

            # å¦‚æœæ”¹è¿›ï¼Œåˆ™é‡‡ç”¨
            if self.evaluate(new_prompt, task) > performance:
                prompt = new_prompt

        return prompt

    def suggest_edits(self, prompt, performance):
        return [
            self.add_examples(prompt),
            self.rephrase_instruction(prompt),
            self.add_constraints(prompt),
            self.simplify(prompt)
        ]
```

**ä»£è¡¨æ–¹æ³•**ï¼š
- GPS, GrIPS, TEMPERA, Plum

**2. è¿›åŒ–å¼ (Evolutionary)**

```python
# EvoPrompt
class EvolutionaryPromptOptimizer:
    def optimize(self, task, population_size=20):
        # åˆå§‹åŒ–ç§ç¾¤
        population = self.initialize_population(population_size)

        for generation in range(50):
            # è¯„ä¼°æ¯ä¸ªä¸ªä½“
            fitness = [self.evaluate(p, task) for p in population]

            # é€‰æ‹©
            parents = self.select_parents(population, fitness)

            # äº¤å‰
            offspring = self.crossover(parents)

            # å˜å¼‚
            offspring = self.mutate(offspring)

            # æ›´æ–°ç§ç¾¤
            population = self.next_generation(population, offspring, fitness)

        return max(population, key=lambda p: self.evaluate(p, task))

    def crossover(self, parents):
        # ç»„åˆä¸¤ä¸ª prompt çš„ä¼˜ç‚¹
        p1, p2 = parents
        return f"{p1[:len(p1)//2]} {p2[len(p2)//2:]}"

    def mutate(self, prompt):
        # éšæœºä¿®æ”¹ prompt
        mutations = [
            self.add_adjective,
            self.change_tone,
            self.add_example
        ]
        return random.choice(mutations)(prompt)
```

**ä»£è¡¨æ–¹æ³•**ï¼š
- EvoPrompt, Promptbreeder, GEPA

**3. ç”Ÿæˆå¼ (Generative)**

```python
# OPRO (Optimization by PROmpting)
class GenerativePromptOptimizer:
    def optimize(self, task, meta_llm):
        history = []

        for iteration in range(10):
            # è®© LLM ç”Ÿæˆæ–°çš„ prompt
            new_prompt = meta_llm.generate(
                f"""Based on the following task and previous attempts,
                generate an improved prompt:

                Task: {task}
                History: {history}

                Generate a better prompt:"""
            )

            # è¯„ä¼°æ–° prompt
            performance = self.evaluate(new_prompt, task)

            # è®°å½•å†å²
            history.append({
                'prompt': new_prompt,
                'performance': performance
            })

        # è¿”å›æœ€ä½³ prompt
        return max(history, key=lambda h: h['performance'])['prompt']
```

**ä»£è¡¨æ–¹æ³•**ï¼š
- OPRO (Google), PromptAgent, APE

**4. æ¢¯åº¦å¼ (Gradient-Based)**

```python
# TextGrad
class GradientBasedPromptOptimizer:
    def optimize(self, prompt, task):
        # å°† prompt è§†ä¸ºå¯å¾®åˆ†å˜é‡
        prompt_embedding = self.embed(prompt)

        for iteration in range(100):
            # å‰å‘ä¼ æ’­
            output = self.model(prompt_embedding)
            loss = self.compute_loss(output, task)

            # åå‘ä¼ æ’­ï¼ˆåœ¨ç¦»æ•£ç©ºé—´ï¼‰
            gradient = self.compute_text_gradient(prompt, loss)

            # æ›´æ–° prompt
            prompt = self.update_prompt(prompt, gradient)

        return prompt

    def compute_text_gradient(self, prompt, loss):
        # ç”¨ LLM è§£é‡Šå¦‚ä½•æ”¹è¿›
        return self.llm.generate(
            f"""The current prompt: "{prompt}"
            The loss: {loss}

            How should we modify the prompt to reduce the loss?
            Provide specific textual changes."""
        )
```

**ä»£è¡¨æ–¹æ³•**ï¼š
- TextGrad, REVOLVE, GRAD-SUM

### 4.3 è®°å¿†ç³»ç»Ÿä¼˜åŒ–

**é•¿æœŸè®°å¿†æ¶æ„**

```python
class LongTermMemoryAgent:
    def __init__(self):
        self.working_memory = []  # çŸ­æœŸï¼ˆå½“å‰å¯¹è¯ï¼‰
        self.episodic_memory = []  # æƒ…èŠ‚ï¼ˆè¿‡å»äº¤äº’ï¼‰
        self.semantic_memory = {}  # è¯­ä¹‰ï¼ˆçŸ¥è¯†å›¾è°±ï¼‰
        self.procedural_memory = {}  # è¿‡ç¨‹ï¼ˆæŠ€èƒ½ï¼‰

    def process_experience(self, experience):
        # 1. æ·»åŠ åˆ°å·¥ä½œè®°å¿†
        self.working_memory.append(experience)

        # 2. åˆ¤æ–­é‡è¦æ€§
        if self.is_important(experience):
            # å­˜å…¥æƒ…èŠ‚è®°å¿†
            self.episodic_memory.append(experience)

            # æå–çŸ¥è¯†åˆ°è¯­ä¹‰è®°å¿†
            knowledge = self.extract_knowledge(experience)
            self.semantic_memory.update(knowledge)

            # å­¦ä¹ æŠ€èƒ½åˆ°è¿‡ç¨‹è®°å¿†
            if experience.is_successful:
                skill = self.extract_skill(experience)
                self.procedural_memory[skill.name] = skill

    def retrieve_relevant_memory(self, query):
        # å¤šå±‚æ¬¡æ£€ç´¢
        relevant = {
            'working': self.search_working_memory(query),
            'episodic': self.search_episodic_memory(query),
            'semantic': self.search_semantic_memory(query),
            'procedural': self.search_procedural_memory(query)
        }
        return relevant
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **Memento**: Gist-based compression
  - å‹ç¼©é•¿å¯¹è¯å†å²
  - ä¿ç•™å…³é”®ä¿¡æ¯
  - å‡å°‘ token æ¶ˆè€—

- **M3-Agent**: åŠ¨æ€æ•´åˆ
  - å¤šæ¨¡æ€è®°å¿†
  - è‡ªé€‚åº”æ£€ç´¢
  - å±‚æ¬¡åŒ–ç»„ç»‡

- **Memory-R1**: å¥–åŠ±é©±åŠ¨
  - å¼ºåŒ–å­¦ä¹ é€‰æ‹©è®°å¿†
  - é—å¿˜æ— ç”¨ä¿¡æ¯
  - å¼ºåŒ–é‡è¦è®°å¿†

### 4.4 å·¥å…·ä½¿ç”¨ä¼˜åŒ–

**è‡ªåŠ¨å·¥å…·å­¦ä¹ **

```python
class ToolLearningAgent:
    def __init__(self):
        self.available_tools = {}
        self.tool_usage_history = []

    def learn_new_tool(self, tool):
        # 1. æ¢ç´¢å·¥å…·åŠŸèƒ½
        capabilities = self.explore_tool(tool)

        # 2. ç”Ÿæˆä½¿ç”¨æ–‡æ¡£
        documentation = self.generate_documentation(tool, capabilities)

        # 3. åˆ›å»ºç¤ºä¾‹
        examples = self.generate_examples(tool)

        # 4. æ³¨å†Œå·¥å…·
        self.available_tools[tool.name] = {
            'tool': tool,
            'doc': documentation,
            'examples': examples,
            'success_rate': 0.0
        }

    def select_tool(self, task):
        # åŸºäºä»»åŠ¡é€‰æ‹©æœ€ä½³å·¥å…·
        candidates = []

        for name, info in self.available_tools.items():
            # è®¡ç®—å·¥å…·ä¸ä»»åŠ¡çš„åŒ¹é…åº¦
            relevance = self.compute_relevance(task, info)
            success_rate = info['success_rate']

            score = relevance * success_rate
            candidates.append((name, score))

        # é€‰æ‹©å¾—åˆ†æœ€é«˜çš„
        best_tool = max(candidates, key=lambda x: x[1])[0]
        return self.available_tools[best_tool]['tool']

    def update_tool_performance(self, tool_name, success):
        # æ›´æ–°å·¥å…·æˆåŠŸç‡
        history = self.tool_usage_history
        recent_uses = [h for h in history if h['tool'] == tool_name][-10:]

        success_rate = sum(u['success'] for u in recent_uses) / len(recent_uses)
        self.available_tools[tool_name]['success_rate'] = success_rate
```

**ä»£è¡¨æ–¹æ³•**ï¼š

- **ToolLLM**: ç›‘ç£å­¦ä¹ å·¥å…·æŒæ¡
- **ReTool**: å¼ºåŒ–å­¦ä¹ ç­–ç•¥é€‰æ‹©
- **CREATOR**: å·¥å…·åŠŸèƒ½è¿›åŒ–
- **Alita**: è‡ªåŠ¨å·¥å…·ç»„åˆ

---

## 5. å¼€æºæ¡†æ¶

### 5.1 EvoAgentX (EMNLP'25)

**æ ¸å¿ƒåŠŸèƒ½**: è‡ªåŠ¨åŒ–è¿›åŒ– Agent å·¥ä½œæµ

```python
# EvoAgentX ä½¿ç”¨ç¤ºä¾‹
from evoagentx import EvolvingWorkflow

# å®šä¹‰åˆå§‹å·¥ä½œæµ
workflow = EvolvingWorkflow(
    task="Develop a web application",
    agents=[
        PMAgent(),
        ArchitectAgent(),
        DeveloperAgent(),
        QAAgent()
    ]
)

# è‡ªåŠ¨ä¼˜åŒ–å·¥ä½œæµ
for iteration in range(10):
    # æ‰§è¡Œä»»åŠ¡
    result = workflow.execute(task_instance)

    # æ”¶é›†åé¦ˆ
    feedback = workflow.collect_feedback(result)

    # è‡ªåŠ¨æ”¹è¿›
    workflow.evolve(feedback)

    print(f"Iteration {iteration}: Success rate = {workflow.success_rate}")

# æœ€ç»ˆå·¥ä½œæµå¯èƒ½å˜æˆï¼š
# PMAgent â†’ RequirementsAnalyzer â†’ ArchitectAgent â†’
# SecurityReviewer â†’ DeveloperAgent â†’ CodeReviewer â†’
# QAAgent â†’ PerformanceTester
```

**ç‰¹ç‚¹**ï¼š
- è‡ªåŠ¨å‘ç°å·¥ä½œæµç“¶é¢ˆ
- åŠ¨æ€æ·»åŠ /åˆ é™¤ Agent
- ä¼˜åŒ– Agent åä½œé¡ºåº

### 5.2 MASLab

**æ ¸å¿ƒåŠŸèƒ½**: å¤š Agent ç³»ç»Ÿç»Ÿä¸€ä»£ç åº“

```python
# MASLab ä½¿ç”¨ç¤ºä¾‹
from maslab import MultiAgentSystem, Agent

class ResearchAgent(Agent):
    def execute(self, task):
        # ç ”ç©¶ä»»åŠ¡
        findings = self.search_papers(task.query)
        return findings

class SynthesisAgent(Agent):
    def execute(self, findings):
        # ç»¼åˆç ”ç©¶ç»“æœ
        summary = self.synthesize(findings)
        return summary

# åˆ›å»ºå¤š Agent ç³»ç»Ÿ
mas = MultiAgentSystem()
mas.add_agent('researcher', ResearchAgent())
mas.add_agent('synthesizer', SynthesisAgent())

# å®šä¹‰å·¥ä½œæµ
mas.define_workflow([
    ('researcher', 'synthesizer')
])

# æ‰§è¡Œ
result = mas.run(task="Latest advances in self-evolving agents")
```

### 5.3 å…¶ä»–é‡è¦æ¡†æ¶

**AutoGen (Microsoft)**

```python
from autogen import AssistantAgent, UserProxyAgent

# åˆ›å»ºåŠ©æ‰‹
assistant = AssistantAgent("assistant")

# åˆ›å»ºç”¨æˆ·ä»£ç†
user_proxy = UserProxyAgent("user_proxy", human_input_mode="NEVER")

# å¯åŠ¨å¯¹è¯
user_proxy.initiate_chat(
    assistant,
    message="Write a Python function to calculate Fibonacci numbers"
)
```

**MetaGPT**

```python
from metagpt.roles import ProductManager, Architect, Engineer

# åˆ›å»ºå›¢é˜Ÿ
team = Team([
    ProductManager(),
    Architect(),
    Engineer()
])

# æ‰§è¡Œé¡¹ç›®
await team.run("Create a task management app")
```

---

## 6. å®é™…åº”ç”¨åœºæ™¯

### 6.1 è‡ªåŠ¨è½¯ä»¶å¼€å‘

**åœºæ™¯**: ä»éœ€æ±‚åˆ°ä»£ç çš„å…¨æµç¨‹è‡ªåŠ¨åŒ–

```
User: "åˆ›å»ºä¸€ä¸ªå¾…åŠäº‹é¡¹åº”ç”¨"
    â†“
PM Agent: åˆ†æéœ€æ±‚ â†’ ç”Ÿæˆ PRD
    â†“
Architect Agent: è®¾è®¡ç³»ç»Ÿæ¶æ„ â†’ é€‰æ‹©æŠ€æœ¯æ ˆ
    â†“
Developer Agent: ç¼–å†™ä»£ç 
    â”œâ”€ Frontend: React + TypeScript
    â”œâ”€ Backend: Node.js + Express
    â””â”€ Database: PostgreSQL
    â†“
QA Agent: æµ‹è¯•ä»£ç 
    â”œâ”€ å•å…ƒæµ‹è¯•
    â”œâ”€ é›†æˆæµ‹è¯•
    â””â”€ E2E æµ‹è¯•
    â†“
Developer Agent: ä¿®å¤ bug
    â†“
DevOps Agent: éƒ¨ç½²åˆ° Vercel
    â†“
å®Œæˆï¼è¿è¡Œä¸­çš„åº”ç”¨
```

**è‡ªè¿›åŒ–ä½“ç°**ï¼š
- ç¬¬ 1 æ¬¡å¼€å‘ï¼šéœ€è¦ 10 ä¸ªè¿­ä»£
- ç¬¬ 5 æ¬¡å¼€å‘ï¼šéœ€è¦ 5 ä¸ªè¿­ä»£ï¼ˆå­¦ä¼šäº†å¸¸è§æ¨¡å¼ï¼‰
- ç¬¬ 20 æ¬¡å¼€å‘ï¼šéœ€è¦ 2 ä¸ªè¿­ä»£ï¼ˆæŒæ¡äº†æœ€ä½³å®è·µï¼‰

### 6.2 æ•°å­¦æ¨ç†

**åœºæ™¯**: è‡ªåŠ¨å®šç†è¯æ˜

```python
# MA-LoT (Multi-Agent Logic-of-Thought)
class TheoremProver:
    def prove(self, theorem):
        # åˆ†è§£ä¸ºå­é—®é¢˜
        subproblems = self.decompose(theorem)

        # å¤š Agent å¹¶è¡Œæ±‚è§£
        results = []
        for subproblem in subproblems:
            agent = self.assign_agent(subproblem)
            result = agent.solve(subproblem)
            results.append(result)

        # æ•´åˆè¯æ˜
        proof = self.integrate_proofs(results)

        # éªŒè¯
        if self.verify(proof):
            return proof
        else:
            # ä»å¤±è´¥ä¸­å­¦ä¹ 
            self.learn_from_failure(proof)
            return self.prove(theorem)  # é‡è¯•
```

### 6.3 ç§‘å­¦ç ”ç©¶

**åœºæ™¯**: è‡ªåŠ¨å®éªŒè®¾è®¡å’Œåˆ†æ

```
Research Question: "å¦‚ä½•æé«˜å¤ªé˜³èƒ½ç”µæ± æ•ˆç‡ï¼Ÿ"
    â†“
Literature Review Agent:
    - æ£€ç´¢ç›¸å…³è®ºæ–‡
    - æ€»ç»“ç°æœ‰æ–¹æ³•
    - è¯†åˆ«ç ”ç©¶ç¼ºå£
    â†“
Hypothesis Agent:
    - åŸºäºæ–‡çŒ®ç”Ÿæˆå‡è®¾
    - è¯„ä¼°å‡è®¾å¯è¡Œæ€§
    â†“
Experiment Design Agent:
    - è®¾è®¡å®éªŒæ–¹æ¡ˆ
    - é€‰æ‹©ææ–™å’Œå‚æ•°
    â†“
Data Analysis Agent:
    - åˆ†æå®éªŒæ•°æ®
    - ç»Ÿè®¡æ˜¾è‘—æ€§æ£€éªŒ
    â†“
Writing Agent:
    - æ’°å†™ç ”ç©¶è®ºæ–‡
    - ç”Ÿæˆå›¾è¡¨
    â†“
Peer Review Agent:
    - è‡ªæˆ‘å®¡æŸ¥
    - æå‡ºæ”¹è¿›å»ºè®®
```

### 6.4 ä¸ªäººçŸ¥è¯†ç®¡ç†ï¼ˆä¸ä½ çš„é¡¹ç›®ç›´æ¥ç›¸å…³ï¼ï¼‰

**åœºæ™¯**: Personal Knowledge Hub çš„è‡ªè¿›åŒ–

```python
class SelfEvolvingKnowledgeHub:
    def __init__(self):
        self.categorization_agent = CategorizationAgent()
        self.summarization_agent = SummarizationAgent()
        self.retrieval_agent = RetrievalAgent()

    def process_new_content(self, content):
        # 1. è‡ªåŠ¨åˆ†ç±»
        category = self.categorization_agent.classify(content)

        # 2. æå–çŸ¥è¯†
        knowledge = self.summarization_agent.extract(content)

        # 3. å»ºç«‹å…³è”
        related = self.retrieval_agent.find_related(knowledge)

        # 4. æ›´æ–°çŸ¥è¯†å›¾è°±
        self.update_graph(knowledge, related, category)

        # 5. å­¦ä¹ æ”¹è¿›
        if self.user_feedback.is_satisfied:
            self.categorization_agent.learn(content, category)
            self.summarization_agent.improve()

    def evolve_search(self, query, user_clicks):
        # ä»ç”¨æˆ·è¡Œä¸ºå­¦ä¹ 
        relevant_docs = [d for d in search_results if d in user_clicks]
        irrelevant_docs = [d for d in search_results if d not in user_clicks]

        # ä¼˜åŒ–æ£€ç´¢ç­–ç•¥
        self.retrieval_agent.update_ranking(
            query, relevant_docs, irrelevant_docs
        )
```

---

## 7. ä¸ä½ çš„é¡¹ç›®å…³è”

### 7.1 ä½ çš„æµè§ˆå™¨æ ‡ç­¾é¡µåˆ†æ

æ ¹æ®ä½ æ‰“å¼€çš„æ ‡ç­¾é¡µï¼Œä½ æ­£åœ¨æ¢ç´¢ï¼š

1. **AI å¼€å‘æ¡†æ¶** (33 ä¸ªæ ‡ç­¾)
   - Claude Code
   - BMAD Method
   - AI Agent æ¼”è¿›

2. **é‡åŒ–äº¤æ˜“** (9 ä¸ªæ ‡ç­¾)
   - vnpy, freqtrade, Hummingbot

3. **åˆ›ä¸š** (10 ä¸ªæ ‡ç­¾)
   - ä¸€äººä¼ä¸š
   - ç‹¬ç«‹å¼€å‘

4. **Personal Knowledge Hub** (9 ä¸ªæ ‡ç­¾)
   - localhost:3000

### 7.2 å¦‚ä½•åº”ç”¨ Self-Evolving Agents åˆ°ä½ çš„é¡¹ç›®

#### åœºæ™¯ 1: Personal Knowledge Hub è‡ªè¿›åŒ–

```python
# apps/web/lib/evolving-knowledge-agent.ts
export class EvolvingKnowledgeAgent {
  private categorizer: Agent;
  private summarizer: Agent;
  private recommender: Agent;

  async processBookmark(bookmark: Bookmark) {
    // 1. è‡ªåŠ¨åˆ†ç±»ï¼ˆåˆå§‹å¯èƒ½ä¸å‡†ç¡®ï¼‰
    const category = await this.categorizer.classify(bookmark);

    // 2. æå–æ‘˜è¦
    const summary = await this.summarizer.extract(bookmark.content);

    // 3. æ¨èç›¸å…³å†…å®¹
    const related = await this.recommender.find(bookmark);

    // 4. æ”¶é›†ç”¨æˆ·åé¦ˆ
    const feedback = await this.getUserFeedback(category, summary);

    // 5. è‡ªæˆ‘æ”¹è¿›
    if (feedback.category_wrong) {
      await this.categorizer.learn(bookmark, feedback.correct_category);
    }
    if (feedback.summary_poor) {
      await this.summarizer.improve(bookmark, feedback.better_summary);
    }

    return { category, summary, related };
  }
}
```

#### åœºæ™¯ 2: é‡åŒ–äº¤æ˜“ç­–ç•¥è‡ªè¿›åŒ–

```python
# ç»“åˆä½ å¯¹é‡åŒ–äº¤æ˜“çš„å…´è¶£
class SelfEvolvingTradingAgent:
    def __init__(self):
        self.strategy_generator = StrategyGenerator()
        self.backtester = Backtester()
        self.optimizer = StrategyOptimizer()

    def evolve_strategy(self, market_data):
        # 1. ç”Ÿæˆåˆå§‹ç­–ç•¥
        strategy = self.strategy_generator.create()

        for generation in range(100):
            # 2. å›æµ‹
            performance = self.backtester.run(strategy, market_data)

            # 3. å¦‚æœè¡¨ç°è‰¯å¥½ï¼Œä¿å­˜
            if performance.sharpe_ratio > 2.0:
                self.save_strategy(strategy)

            # 4. è¿›åŒ–ç­–ç•¥
            strategy = self.optimizer.evolve(strategy, performance)

        return self.get_best_strategy()
```

#### åœºæ™¯ 3: ä¸€äººå…¬å¸å·¥ä½œæµè‡ªåŠ¨åŒ–

```python
# ç»“åˆä½ çš„åˆ›ä¸šå…´è¶£
class OnepersonBusinessAgent:
    def __init__(self):
        self.content_agent = ContentCreator()
        self.marketing_agent = MarketingAutomation()
        self.customer_agent = CustomerSupport()

    def automate_business(self):
        # å†…å®¹åˆ›ä½œ
        blog_post = self.content_agent.write("æœ€æ–° AI è¶‹åŠ¿")

        # è‡ªåŠ¨å‘å¸ƒ
        self.marketing_agent.publish(blog_post, platforms=['Twitter', 'LinkedIn'])

        # å®¢æˆ·æ”¯æŒ
        for query in self.customer_agent.get_queries():
            response = self.customer_agent.answer(query)
            self.customer_agent.send(response)

        # ä»åé¦ˆä¸­å­¦ä¹ 
        feedback = self.get_customer_feedback()
        self.content_agent.learn(feedback)
        self.customer_agent.improve(feedback)
```

### 7.3 å®æ–½è·¯çº¿å›¾

**ç¬¬ 1 é˜¶æ®µï¼šåŸºç¡€é›†æˆ (1-2 å‘¨)**

- [ ] é›†æˆä¸€ä¸ªç®€å•çš„è‡ªè¿›åŒ– Agent
  ```typescript
  // apps/web/lib/simple-agent.ts
  import { OpenAI } from 'openai';

  export class SimpleEvolvingAgent {
    private memory: Map<string, any> = new Map();

    async improve(task: string, feedback: Feedback) {
      // å­˜å‚¨æˆåŠŸçš„æ¨¡å¼
      if (feedback.success) {
        this.memory.set(task, feedback.solution);
      }

      // ä¸‹æ¬¡é‡åˆ°ç±»ä¼¼ä»»åŠ¡æ—¶ï¼Œå…ˆæŸ¥è®°å¿†
      return this.memory.get(task);
    }
  }
  ```

**ç¬¬ 2 é˜¶æ®µï¼šè®°å¿†ç³»ç»Ÿ (2-3 å‘¨)**

- [ ] å®ç°é•¿æœŸè®°å¿†
- [ ] çŸ¥è¯†å›¾è°±æ„å»º
- [ ] è‡ªåŠ¨åˆ†ç±»ä¼˜åŒ–

**ç¬¬ 3 é˜¶æ®µï¼šå¤š Agent åä½œ (1 ä¸ªæœˆ)**

- [ ] ä½¿ç”¨ AutoGen æˆ– MetaGPT
- [ ] æ„å»ºå·¥ä½œæµ
- [ ] è‡ªåŠ¨ä¼˜åŒ–æµç¨‹

**ç¬¬ 4 é˜¶æ®µï¼šé¢†åŸŸç‰¹åŒ– (æŒç»­)**

- [ ] é’ˆå¯¹ä½ çš„éœ€æ±‚å®šåˆ¶
- [ ] æ”¶é›†ç”¨æˆ·åé¦ˆ
- [ ] æŒç»­è¿­ä»£æ”¹è¿›

---

## 8. å­¦ä¹ è·¯å¾„

### å…¥é—¨çº§ï¼ˆ1-2 å‘¨ï¼‰

**ç›®æ ‡**: ç†è§£æ ¸å¿ƒæ¦‚å¿µ

- [ ] **é˜…è¯»è®ºæ–‡**
  1. "Self-Rewarding Language Models" (Meta)
  2. "Tree of Thoughts" (Princeton)
  3. "AutoGen" (Microsoft)

- [ ] **å®è·µé¡¹ç›®**
  1. å®ç°ä¸€ä¸ªç®€å•çš„åé¦ˆå¾ªç¯
  2. ä½¿ç”¨ AutoGen åˆ›å»ºå¯¹è¯ Agent
  3. ä½“éªŒ Prompt ä¼˜åŒ–

**èµ„æº**ï¼š
- GitHub: Awesome-Self-Evolving-Agents
- è®ºæ–‡: arXiv.org
- ä»£ç : AutoGen, LangChain

### ä¸­çº§ï¼ˆ1-2 ä¸ªæœˆï¼‰

**ç›®æ ‡**: æŒæ¡æ ¸å¿ƒæŠ€æœ¯

- [ ] **æ·±å…¥ç†è§£**
  1. RL åŸºç¡€ï¼ˆå¼ºåŒ–å­¦ä¹ ï¼‰
  2. Prompt Engineering é«˜çº§æŠ€å·§
  3. å¤š Agent ç³»ç»Ÿè®¾è®¡

- [ ] **æ„å»ºé¡¹ç›®**
  1. ä¸ªäººçŸ¥è¯†åŠ©æ‰‹ï¼ˆè‡ªè¿›åŒ–ï¼‰
  2. ä»£ç ç”Ÿæˆ Agentï¼ˆæŒç»­æ”¹è¿›ï¼‰
  3. ç ”ç©¶åŠ©æ‰‹ï¼ˆå¤š Agent åä½œï¼‰

**èµ„æº**ï¼š
- è¯¾ç¨‹: Hugging Face RL Course
- æ¡†æ¶: LangChain, AutoGen, DSPy
- ç¤¾åŒº: Discord, GitHub Discussions

### é«˜çº§ï¼ˆ3-6 ä¸ªæœˆï¼‰

**ç›®æ ‡**: åˆ›æ–°å’Œè´¡çŒ®

- [ ] **ç ”ç©¶æ–¹å‘**
  1. è®¾è®¡æ–°çš„è¿›åŒ–ç®—æ³•
  2. å‘è¡¨è®ºæ–‡æˆ–å¼€æºé¡¹ç›®
  3. å•†ä¸šåŒ–åº”ç”¨

- [ ] **é¢†åŸŸæ·±åŒ–**
  1. é€‰æ‹©ç‰¹å®šé¢†åŸŸï¼ˆå¦‚é‡åŒ–äº¤æ˜“ï¼‰
  2. æ„å»ºä¸“ä¸šç³»ç»Ÿ
  3. ä¼˜åŒ–åˆ°ç”Ÿäº§çº§åˆ«

**èµ„æº**ï¼š
- ä¼šè®®: NeurIPS, ICML, EMNLP
- ç«èµ›: Kaggle, Hugging Face
- åˆä½œ: å¼€æºè´¡çŒ®

---

## ğŸ¯ æ€»ç»“

### æ ¸å¿ƒè¦ç‚¹

1. **Self-Evolving Agents æ˜¯ AI çš„æœªæ¥**
   - ä»é™æ€åˆ°åŠ¨æ€
   - ä»å›ºå®šåˆ°é€‚åº”
   - ä»è¢«åŠ¨åˆ°ä¸»åŠ¨

2. **ä¸‰å¤§æ”¯æŸ±**
   - å• Agent ä¼˜åŒ–
   - å¤š Agent åä½œ
   - é¢†åŸŸç‰¹åŒ–

3. **å…³é”®æŠ€æœ¯**
   - LLM è¡Œä¸ºä¼˜åŒ–ï¼ˆè®­ç»ƒ+æµ‹è¯•æ—¶ï¼‰
   - Prompt å·¥ç¨‹ï¼ˆ4 ç§èŒƒå¼ï¼‰
   - è®°å¿†ç³»ç»Ÿï¼ˆé•¿æœŸå­¦ä¹ ï¼‰
   - å·¥å…·å­¦ä¹ ï¼ˆè‡ªåŠ¨æŒæ¡ï¼‰

4. **å®é™…åº”ç”¨**
   - è½¯ä»¶å¼€å‘è‡ªåŠ¨åŒ–
   - ç§‘å­¦ç ”ç©¶åŠ é€Ÿ
   - çŸ¥è¯†ç®¡ç†æ™ºèƒ½åŒ–
   - ä¸šåŠ¡æµç¨‹ä¼˜åŒ–

### å¯¹ä½ çš„ä»·å€¼

åŸºäºä½ çš„å…´è¶£ï¼ˆAI å¼€å‘ã€é‡åŒ–äº¤æ˜“ã€åˆ›ä¸šï¼‰ï¼ŒSelf-Evolving Agents å¯ä»¥å¸®ä½ ï¼š

1. **Personal Knowledge Hub**
   - è‡ªåŠ¨åˆ†ç±»å’Œæ€»ç»“
   - æ™ºèƒ½æ¨è
   - æŒç»­ä¼˜åŒ–æ£€ç´¢

2. **é‡åŒ–äº¤æ˜“**
   - ç­–ç•¥è‡ªåŠ¨ç”Ÿæˆ
   - å›æµ‹å’Œä¼˜åŒ–
   - é€‚åº”å¸‚åœºå˜åŒ–

3. **ä¸€äººå…¬å¸**
   - å†…å®¹è‡ªåŠ¨åˆ›ä½œ
   - å®¢æˆ·æ”¯æŒè‡ªåŠ¨åŒ–
   - è¥é”€æµç¨‹ä¼˜åŒ–

### ä¸‹ä¸€æ­¥è¡ŒåŠ¨

```
1. æ·±å…¥é˜…è¯» Awesome-Self-Evolving-Agents
   â””â”€ é€‰æ‹© 3-5 ç¯‡æ ¸å¿ƒè®ºæ–‡

2. ä½“éªŒå¼€æºæ¡†æ¶
   â””â”€ AutoGen æˆ– MetaGPT

3. é›†æˆåˆ°ä½ çš„é¡¹ç›®
   â””â”€ Personal Knowledge Hub æ·»åŠ è‡ªè¿›åŒ–åŠŸèƒ½

4. æŒç»­å­¦ä¹ 
   â””â”€ å…³æ³¨æœ€æ–°ç ”ç©¶å’Œå®è·µ
```

---

**è¿™ä¸ªé¡¹ç›®ä»£è¡¨äº† AI Agent çš„å‰æ²¿æ–¹å‘ï¼Œå€¼å¾—æ·±å…¥ç ”ç©¶ï¼** ğŸš€

**éœ€è¦æˆ‘å¸®ä½ **ï¼š
- è¯¦ç»†è§£é‡ŠæŸä¸ªå…·ä½“æŠ€æœ¯ï¼Ÿ
- å¸®ä½ è®¾è®¡ Personal Knowledge Hub çš„è‡ªè¿›åŒ–åŠŸèƒ½ï¼Ÿ
- åˆ†ææŸç¯‡è®ºæ–‡ï¼Ÿ
- å®ç°ä¸€ä¸ª Demoï¼Ÿ

å‘Šè¯‰æˆ‘ä½ æœ€æ„Ÿå…´è¶£çš„éƒ¨åˆ†ï¼
